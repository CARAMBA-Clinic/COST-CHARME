# Metabolomics data analysis using Pachyderm
In this page we introduce an metabolomics preprocessing workflow that you can run using [Pachyderm](https://github.com/pachyderm/pachyderm), a distributed data-processing tool built on software containers that enables scalable and reproducible pipelines.

## Requirements
You need to install Vagrant and Virtualbox.

In Windows:

- Install Chocolate: https://chocolatey.org/install

- Install VirtualBox: https://www.virtualbox.org/wiki/Downloads

- Open PowerShell and run: 
```bash
> choco install vagrant
```
In Mac & linux: 

You should be able to download the installation files for your operating system:

- Install Vagrant: https://www.vagrantup.com/downloads.html
- Install VirtualBox: https://www.virtualbox.org/wiki/Downloads

## Set up the vagrant box
Using PowerShell or Terminal, navigate to the folder containing the box file and run:
```bash
> vagrant up --provision
```
SSH into the machine 
```bash
> vagrant ssh
```
Check jupyter notebook and Rstudio:
Open a web browser and navigate to [localhost:8888](localhost:8888) for jupyter notebook and [localhost:8787](localhost:8787) for Rstudio.

## Hands-on with Pachyderm

### Useful information
The most common way to interact with Pachyderm is by using the Pachyderm Client (pachctl). You can explore the different commands available by using:
```bash
> pachctl  --help
```
And if you need more information about a particular command please use:
```bash
> pachctl <name of the command> --help
```
### Pushing data into Pachyderm's Data Repository
A repo is the highest level data primitive in Pachyderm. They should be dedicated to a single source of data such as the input from a particular tool. Examples include training data for an ML model or genome annotation data.
Here we will create a single repo which will serve as input for the first step of the workflow:
```bash
> pachctl create-repo mrpo
```
You can push data into this repository using the put-file command. This will create a new commit, add data, and finish the commit. Explore further on how commits work.
```bash
> cd ./MSData
```
```bash
> pachctl put-file <name of the repo> <name of the branch> -c -r -p <number of files to upload in parallel> -f .
```
### Running a Pachyderm pipeline
Once your data is in the repository, you are ready to start a bunch of pipelines cranking through data in a distributed fashion. Pipelines are the core processing primitive in Pachyderm and they’re specified with a JSON encoding. Explore the pipelines folder and find out which of the pipelines is the first step of the pre-processing workflow. You can find it by discovering which pipeline has the previously created repository as an input. Then run it using:
```bash
> pachctl create-pipeline -f <JSON file>
```
What happens after you create a pipeline? Creating a pipeline tells Pachyderm to run your code on every finished commit in a repo as well as all future commits that happen after the pipeline is created. Our repo already had a commit, so Pachyderm automatically launched a job (Kubernetes pod) to process that data. This first time it might take some extra time since it needs to download the image from a container image registry. You can view the pipeline status and its corresponding jobs using:
```bash
> pachctl list-job
```
and 

```bash
> pachctl list-pipeline
```
And explore the different worker pods in your Kubernetes cluster via:
```bash
> kubectl get pods -o wide
```
Try changing some parameters such as the parallelism specification, resource specification and glob pattern. What is happening? How many pods are scheduled? Play with the parameters and see the differences. You can learn about the different settings here: http://docs.pachyderm.io/en/v1.5.0/reference/pipeline_spec.html

You can re-run the pipeline with a new pipeline definition (new parameters etc) like this:
```bash
> pachctl update-pipeline -f <JSON file>
```
Three more pipelines compose the pre-processing workflow. After you run the entire workflow, the resulting CSV file generated by the TextExporter in OpenMS will be saved in the TextExporter repository. You can download the file simply by using:
```bash
> pachctl get-file TextExporter <commit-id> <path-to-file-in pachd> > <custom-name-of-file>
```
The <commit-id> is easily obtainable by checking the most recently made commit in the TextExporter repository using:
```bash
> pachctl list-commit TextExporter
```
Also, the <path-to-file> can be obtained by checking the list of files outputted to the TextExporter repository at a specific branch. To which branch does Pachyderm make commits by default?
```bash
> pachctl list-file <name-of-repo> <branch-name>
```
### Data versioning in Pachyderm
Pachyderm uses a Data Repository within its File System. This means that it will keep track of different file versions over time, like Git. Effectively, it enables the ability to track the provenance of results: results can be traced back to their origins at any time point.

Pipelines automatically process the data as new commits are finished. Think of pipelines as being subscribed to any new commits on their input repositories. Similarly to Git, commits have a parental structure that tracks which files have changed. In this case we are going add some more metabolite data files.

Let’s create a new commit in a parental structure. To do this we will simply do two more put-file commands with -c and by specifying master as the branch, it will automatically parent our commits onto each other. Branch names are just references to a particular HEAD commit.
```bash
> cd ./MSDataNew
```
```bash
> pachctl put-file <name of the repo> <name of the branch> -c -r -p <number of files to upload in parallel> -f .
```
Did any new job get triggered? What data is being processed now? All available data or just new data? Explore which new commits have been made as a result of the new input data. 
```bash
> pachctl list-commit <repo-name>
```
## How to develop a simple R-based microservice

When migrating your monolitic workflow to a microservice-based infrastructure, you will have to split it in smaller, interchangeable, tasks. This is anyway a general good practice to follow when you develop your software, that will promote *separation of concern* and reusability. 

Here we propose a microservice infrastructure based on MANTL. In MANTL, complex applications are built deploying Docker containers that act as microservices. When separating your application in Docker containers, it is important to find a good strategy to share data between them. In MANTL, [GlusterFS](https://www.gluster.org/) is used to provide a distributed filesystem where containers, that can potentially run on different nodes, can share data. Therefore, you can assume that each microservice in a complex workflow reads the input, and it writes the output, form some volume that will be mounted by MANTL. 

Here we use a workflow by the Kultima lab as benchmark. In this tutorial you will deploy your very own MANTL cluster, and run this workflow using an interactive Jupyter [notebook](https://github.com/phnmnl/workflow-demo/blob/master/Jupyter/Workflow.ipynb). Please give a quick look to it before to proceed with the next section. 

### Develop microservices with Docker

In this section we show how to wrap a simple R-script in a Docker image, that can act as a microservice in a more complex workflow. For the best learning experience, we recommend that you repeat every step on your own.  

Here we use one of the smallest services in the benchmark pipeline, the [log2transformation](https://github.com/phnmnl/workflow-demo/tree/master/log2transformation). This process will take intensity data as an input, and transform it to the log2 base scale. The missing values will be further imputed by zeros. Please notice that in this R script, the data is read/write from/to the disk.

```R
args <- commandArgs(trailingOnly = TRUE)

input = args[1]
output = args[2]
samples<-read.table(input,sep='\t',header=T)

samples=log2(samples)
samples[is.na(samples)]=0

write.table(samples,file=output,sep='\t',row.names=F)
```

All you need to do in order to wrap this script in a Docker image is to write a [Dockerfile](https://docs.docker.com/engine/reference/builder/). An example follows.

```Docker
FROM r-base
MAINTAINER Stephanie Herman, stephanie.herman.3820@student.uu.se

ADD log2transformation.r /
ENTRYPOINT ["Rscript", "log2transformation.r"]
```

In the Dockerfile you first specify a base image that you want to start **FROM**. If you are working to an R-based service, like we are doing, the base image *r-base* is a good choice, as it includes all of the dependencies you need to run your script. Then, you provide the **MAINTAINER**, that is typically your name and a contact.

The last two lines in our simple Docker file are the most important. The **ADD** instruction serves to add a file in the build context to a directory in your Docker image. In fact, we use it to add our *log2transformation.r* script in the root directory. Finally, the *ENTRYPOINT* instruction, specifies which command to run when the container will be started. Of course, we use it to run our script.

When you are done with the Dockerfile, you need to build the image. The `docker build` command does the job. 

```
$ docker build -t log2transformation .
```

In the previous command we build the image, naming it *log2transformation*, and specifying the current directory as the build context. To successfully run this command, it is very important that the build context, the current directory, contains both the *Dockerfile* and the *log2transformation.r* script. If everything works fine it will say that the image was successfully built.

The `docker run` command serves to run a service that has been previously built. You can use this [input data](https://raw.githubusercontent.com/phnmnl/workflow-demo/master/data/log2_input.xls) to try out the following command.

```
$ docker run -v /host/directory/data:/data log2transformation /data/log2_input.xls /data/log2_output.xls
```

In the previous command we use the `-v` argument to specify a directory on our host machine, that will be mount on the Docker container. This directory is supposed to contain the [log2_input.xls](https://raw.githubusercontent.com/phnmnl/workflow-demo/master/data/log2_input.xls) file. Then we specify the name of the container that we aim to run (*log2transformation*), and the arguments that will be passed to the entry point command. We mounted the host direcory under */data* in the Docker container, hence we use the arguments to instruct the R script to read/write the input from/to it.    

You can read more on how to develop Docker images on the Docker [documentation](https://docs.docker.com/). 
